# Cosas a tener en cuenta para modelar la base de datos a partir del mermaid

## CON RESPECTO A LAS CONSULTAS (EVALUACI√ìN)

Uno de los objetivos que se necesita en una consulta que se hace continuamente es:

### Objetivo

Para cada combinaci√≥n de ubicaci√≥n y variable, quieres:

- Obtener las 3 √∫ltimas lecturas (ordenadas por datetime)

- Calcular la media de esas 3 lecturas (valor)

- Hacerlo aunque haya m√°s o menos de 3 registros

- Si hay menos de 3, se usa la media de los disponibles.

Esa es una de las consultas mas importantes.

Basicamente Analisis complejo de eventos.


**Consultas que quizas funcionan**
`LECTURA` Es un snapshot de las lecturas que hay en una planta, ya estan ordenadas por ubicaci√≥n que es algo que interesa, se puede hacer una consulta para obtenerlas ordenadas por ubicaci√≥n.

A continuaci√≥n unas consultas que quizas sirven para cumplir el objetivo.
```sql
WITH ordenadas AS (
  SELECT
    id_ubicacion,
    id_variable,
    valor,
    datetime,
    ROW_NUMBER() OVER (
      PARTITION BY id_ubicacion, id_variable
      ORDER BY datetime DESC
    ) AS rn
  FROM LECTURA
),
ultimas_3 AS (
  SELECT *
  FROM ordenadas
  WHERE rn <= 3
)
SELECT
  id_ubicacion,
  id_variable,
  ROUND(AVG(valor), 2) AS media_3_ultimas
FROM ultimas_3
GROUP BY id_ubicacion, id_variable;
```

```sql
SELECT id_sensor, id_variable, id_ubicacion,
       AVG(valor) AS media_ultimos_3
FROM (
    SELECT id_sensor, id_variable, id_ubicacion, valor,
           ROW_NUMBER() OVER (
               PARTITION BY id_sensor, id_variable, id_ubicacion
               ORDER BY timestamp DESC
           ) as rn
    FROM HISTORICO_LECTURAS
) sub
WHERE rn <= 3
GROUP BY id_sensor, id_variable, id_ubicacion;
```
Este c√°lculo lo puedes aplicar dentro de un procedimiento o script que se ejecute cada 10 segundos. Tambi√©n puedes usar una vista materializada si usas PostgreSQL o engines que lo soporten.

---

¬øQu√© es lo que quieres lograr?
Tener acceso r√°pido y directo a los √∫ltimos datos (estado actual de sensores, variables, ubicaciones‚Ä¶).

No perder los datos antiguos, porque son necesarios para auditor√≠as, estad√≠sticas o evaluaciones hist√≥ricas.

--- 

## CON RESPECTO A LAS TABLAS LECTURA HISTORICO_LECTURA

Estrategia recomendada (rendimiento + agrupamiento)
Ya que:

Prioridad = rendimiento muy alto.

Las balizas generan muchas lecturas en paralelo.

Necesitas procesarlas agrupadas por ubicaci√≥n.

Te interesa evaluar el estado actual, pero tambi√©n tener trazabilidad.

La mejor opci√≥n es:

‚úÖ Mantener dos tablas:
LECTURA_ACTUAL: contiene solo la √∫ltima lectura por sensor + variable + ubicaci√≥n.

LECTURA_HISTORICO: contiene todas las lecturas, sirve para auditor√≠a y estad√≠sticas.

üß± Estructura sugerida

LECTURA_ACTUAL
```sql
CREATE TABLE LECTURA_ACTUAL (
  id_lectura INT PRIMARY KEY AUTO_INCREMENT,
  id_ubicacion INT NOT NULL,
  id_baliza INT NOT NULL,
  id_sensor INT NOT NULL,
  id_variable INT NOT NULL,
  datetime TIMESTAMP NOT NULL,
  valor FLOAT,
  UNIQUE KEY idx_sensor_variable_ubicacion (id_sensor, id_variable, id_ubicacion),
  FOREIGN KEY (id_ubicacion) REFERENCES UBICACION(id_ubicacion),
  FOREIGN KEY (id_baliza) REFERENCES BALIZA(id_baliza),
  FOREIGN KEY (id_sensor) REFERENCES SENSOR(id_sensor),
  FOREIGN KEY (id_variable) REFERENCES VARIABLE(id_variable)
);
```

LECTURA_HISTORICO
```sql
CREATE TABLE LECTURA_HISTORICO (
  id INT PRIMARY KEY AUTO_INCREMENT,
  id_ubicacion INT NOT NULL,
  id_baliza INT NOT NULL,
  id_sensor INT NOT NULL,
  id_variable INT NOT NULL,
  datetime TIMESTAMP NOT NULL,
  valor FLOAT,
  INDEX idx_ubicacion_datetime (id_ubicacion, datetime),
  FOREIGN KEY (id_ubicacion) REFERENCES UBICACION(id_ubicacion),
  FOREIGN KEY (id_baliza) REFERENCES BALIZA(id_baliza),
  FOREIGN KEY (id_sensor) REFERENCES SENSOR(id_sensor),
  FOREIGN KEY (id_variable) REFERENCES VARIABLE(id_variable)
);
```
### insercion de datos
Inserci√≥n de datos (batch o trigger)
Cuando llegan nuevas lecturas:

Se insertan en LECTURA_HISTORICO.

Se actualiza o reemplaza el valor de LECTURA_ACTUAL.

Esto puedes hacerlo desde tu backend o mediante un TRIGGER (menos flexible para datos masivos, pero funcional).

Ejemplo en backend (pseudoc√≥digo SQL)
```sql
-- 1. Insertar en hist√≥rico
INSERT INTO LECTURA_HISTORICO (id_ubicacion, id_baliza, id_sensor, id_variable, datetime, valor)
VALUES (..., ..., ..., ..., ..., ...);

-- 2. Reemplazar la lectura actual
INSERT INTO LECTURA_ACTUAL (id_ubicacion, id_baliza, id_sensor, id_variable, datetime, valor)
VALUES (..., ..., ..., ..., ..., ...)
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime);
El ON DUPLICATE KEY UPDATE funciona porque tienes la restricci√≥n UNIQUE (id_sensor, id_variable, id_ubicacion).
```
---

### consultas

Agrupamiento por ubicaci√≥n
Ahora puedes hacer consultas como:

```sql
-- √öltimas lecturas agrupadas por ubicaci√≥n
SELECT 
  id_ubicacion, 
  COUNT(*) as total_lecturas, 
  AVG(valor) as promedio_valor
FROM LECTURA_ACTUAL
GROUP BY id_ubicacion;
```
O si necesitas todo el detalle:

```sql
SELECT * 
FROM LECTURA_ACTUAL 
WHERE id_ubicacion = 3;
```
Y si necesitas comparar contra el hist√≥rico:

```sql
SELECT * 
FROM LECTURA_HISTORICO 
WHERE id_ubicacion = 3 
  AND datetime BETWEEN NOW() - INTERVAL 10 MINUTE AND NOW();
```

### particiones

üöÄ Optimizaci√≥n adicional
√çndices en `LECTURA_HISTORICO` por `(id_ubicacion, datetime)` ‚Üí muy √∫til para filtros por ubicaci√≥n y tiempo.

Particionado en LECTURA_HISTORICO:

Por fecha `(RANGE(datetime))`: √∫til si vas a consultar por per√≠odos.

Por ubicaci√≥n `(LIST(id_ubicacion))`: √∫til si consultas ubicaci√≥n por ubicaci√≥n.

Ejemplo:

```sql
CREATE TABLE LECTURA_HISTORICO_PART (
  -- columnas
)
PARTITION BY RANGE (YEAR(datetime)) (
  PARTITION p2024 VALUES LESS THAN (2025),
  PARTITION p2025 VALUES LESS THAN (2026)
);
```
### vistas?
Vistas si quieres un resumen por ubicaci√≥n para evaluaciones de seguridad:

```sql
CREATE VIEW vista_estado_actual_por_ubicacion AS
SELECT id_ubicacion, AVG(valor) as media, MAX(datetime) as ultima
FROM LECTURA_ACTUAL
GROUP BY id_ubicacion;
```

### ‚úÖConclusi√≥n
Tu idea es s√≥lida y la estrategia sugerida es:

LECTURA_ACTUAL: estado actual, consultas r√°pidas.

LECTURA_HISTORICO: almac√©n completo, an√°lisis.

Operaciones r√°pidas con ON DUPLICATE KEY UPDATE.

Agrupamientos por ubicaci√≥n son simples con GROUP BY id_ubicacion.

---
### UNIQUE KEY (id_sensor, id_varaible, id_ubicaci√≥n)
¬øQu√© es UNIQUE KEY (id_sensor, id_variable, id_ubicacion)?
Esta clave √∫nica:

Impide que haya m√°s de un registro con la misma combinaci√≥n de:

Sensor

Variable

Ubicaci√≥n

Se utiliza para que al insertar una lectura nueva del mismo sensor + variable + ubicaci√≥n, se reemplace la anterior sin duplicar.

Adem√°s, crea autom√°ticamente un √≠ndice compuesto, lo que hace que:

Buscar por id_sensor, id_variable y/o id_ubicacion sea mucho m√°s r√°pido.

Las b√∫squedas y actualizaciones sean eficientes (optimizaci√≥n de consultas).


Este √≠ndice:

sql
Copiar c√≥digo
UNIQUE KEY idx_sensor_variable_ubicacion (id_sensor, id_variable, id_ubicacion)
dice: ‚ùó"No puedes tener m√°s de una fila con la combinaci√≥n (1, 1, 10) o (2, 1, 10)."

---

### ejemplo?

Ejemplo completo con dos ubicaciones y varios sensores/variables
Supongamos que tienes:

2 ubicaciones: 10 y 20

2 sensores: 1 y 2

2 variables: 1 (Temperatura) y 2 (Humedad)

üü¢ Tabla LECTURA_ACTUAL con UNIQUE (id_sensor, id_variable, id_ubicacion)
```sql
CREATE TABLE LECTURA_ACTUAL (
    id_lectura INT PRIMARY KEY,
    id_sensor INT,
    id_variable INT,
    id_ubicacion INT,
    valor FLOAT,
    datetime TIMESTAMP,
    UNIQUE (id_sensor, id_variable, id_ubicacion)
);
```
üîÑ Simulaci√≥n de inserciones (usando ON DUPLICATE para mantener actualizadas)
sql
Copiar c√≥digo
-- Lectura inicial para sensor 1, variable 1 en ubicaci√≥n 10
INSERT INTO LECTURA_ACTUAL VALUES (1, 1, 1, 10, 22.0, '2025-05-19 10:00:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Llega una nueva lectura m√°s reciente para esa combinaci√≥n
INSERT INTO LECTURA_ACTUAL VALUES (2, 1, 1, 10, 23.5, '2025-05-19 10:05:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Humedad en ubicaci√≥n 10
INSERT INTO LECTURA_ACTUAL VALUES (3, 1, 2, 10, 65.0, '2025-05-19 10:06:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Temperatura en ubicaci√≥n 20
INSERT INTO LECTURA_ACTUAL VALUES (4, 2, 1, 20, 24.8, '2025-05-19 10:07:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Humedad en ubicaci√≥n 20
INSERT INTO LECTURA_ACTUAL VALUES (5, 2, 2, 20, 60.0, '2025-05-19 10:08:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);
üìä Resultado final de la tabla LECTURA_ACTUAL
id_lectura	id_sensor	id_variable	id_ubicacion	valor	datetime
2	1	1	10	23.5	2025-05-19 10:05:00
3	1	2	10	65.0	2025-05-19 10:06:00
4	2	1	20	24.8	2025-05-19 10:07:00
5	2	2	20	60.0	2025-05-19 10:08:00

üî∏ Como ves: solo hay una fila por cada combinaci√≥n de sensor + variable + ubicaci√≥n.

---

üéØ Objetivo
Para cada combinaci√≥n de ubicaci√≥n y variable, quieres:

Obtener las 3 √∫ltimas lecturas (ordenadas por datetime)

Calcular la media de esas 3 lecturas (valor)

Hacerlo aunque haya m√°s o menos de 3 registros

Si hay menos de 3, se usa la media de los disponibles.

---

### Evaluaciones con ventana deslizante

Ejemplo SQL para evaluar dentro de tu l√≥gica:

```sql
SELECT id_sensor, id_variable, id_ubicacion,
       AVG(valor) AS media_ultimos_3
FROM (
    SELECT id_sensor, id_variable, id_ubicacion, valor,
           ROW_NUMBER() OVER (
               PARTITION BY id_sensor, id_variable, id_ubicacion
               ORDER BY timestamp DESC
           ) as rn
    FROM HISTORICO_LECTURAS
) sub
WHERE rn <= 3
GROUP BY id_sensor, id_variable, id_ubicacion;
```
Este c√°lculo lo puedes aplicar dentro de un procedimiento o script que se ejecute cada 10 segundos. Tambi√©n puedes usar una vista materializada si usas PostgreSQL o engines que lo soporten.

