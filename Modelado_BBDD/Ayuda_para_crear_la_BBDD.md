# Cosas a tener en cuenta para modelar la base de datos a partir del mermaid

## CON RESPECTO A LAS CONSULTAS (EVALUACI√ìN)

Uno de los objetivos que se necesita en una consulta que se hace continuamente es:

### Objetivo

Para cada combinaci√≥n de ubicaci√≥n y variable, quieres:

- Obtener las 3 √∫ltimas lecturas (ordenadas por datetime)

- Calcular la media de esas 3 lecturas (valor)

- Hacerlo aunque haya m√°s o menos de 3 registros

- Si hay menos de 3, se usa la media de los disponibles.

Esa es una de las consultas mas importantes.

Basicamente Analisis complejo de eventos.


**Consultas que quizas funcionan**
`LECTURA` Es un snapshot de las lecturas que hay en una planta, ya estan ordenadas por ubicaci√≥n que es algo que interesa, se puede hacer una consulta para obtenerlas ordenadas por ubicaci√≥n.

A continuaci√≥n unas consultas que quizas sirven para cumplir el objetivo.
```sql
WITH ordenadas AS (
  SELECT
    id_ubicacion,
    id_variable,
    valor,
    datetime,
    ROW_NUMBER() OVER (
      PARTITION BY id_ubicacion, id_variable
      ORDER BY datetime DESC
    ) AS rn
  FROM LECTURA
),
ultimas_3 AS (
  SELECT *
  FROM ordenadas
  WHERE rn <= 3
)
SELECT
  id_ubicacion,
  id_variable,
  ROUND(AVG(valor), 2) AS media_3_ultimas
FROM ultimas_3
GROUP BY id_ubicacion, id_variable;
```

```sql
SELECT id_sensor, id_variable, id_ubicacion,
       AVG(valor) AS media_ultimos_3
FROM (
    SELECT id_sensor, id_variable, id_ubicacion, valor,
           ROW_NUMBER() OVER (
               PARTITION BY id_sensor, id_variable, id_ubicacion
               ORDER BY timestamp DESC
           ) as rn
    FROM HISTORICO_LECTURAS
) sub
WHERE rn <= 3
GROUP BY id_sensor, id_variable, id_ubicacion;
```
Este c√°lculo lo puedes aplicar dentro de un procedimiento o script que se ejecute cada 10 segundos. Tambi√©n puedes usar una vista materializada si usas PostgreSQL o engines que lo soporten.

---

¬øQu√© es lo que quieres lograr?
Tener acceso r√°pido y directo a los √∫ltimos datos (estado actual de sensores, variables, ubicaciones‚Ä¶).

No perder los datos antiguos, porque son necesarios para auditor√≠as, estad√≠sticas o evaluaciones hist√≥ricas.

--- 

## CON RESPECTO A LAS TABLAS LECTURA HISTORICO_LECTURA

Estrategia recomendada (rendimiento + agrupamiento)
Ya que:

Prioridad = rendimiento muy alto.

Las balizas generan muchas lecturas en paralelo.

Necesitas procesarlas agrupadas por ubicaci√≥n.

Te interesa evaluar el estado actual, pero tambi√©n tener trazabilidad.

La mejor opci√≥n es:

‚úÖ Mantener dos tablas:
LECTURA_ACTUAL: contiene solo la √∫ltima lectura por sensor + variable + ubicaci√≥n.

LECTURA_HISTORICO: contiene todas las lecturas, sirve para auditor√≠a y estad√≠sticas.

üß± Estructura sugerida

LECTURA_ACTUAL
```sql
CREATE TABLE LECTURA_ACTUAL (
  id_lectura INT PRIMARY KEY AUTO_INCREMENT,
  id_ubicacion INT NOT NULL,
  id_baliza INT NOT NULL,
  id_sensor INT NOT NULL,
  id_variable INT NOT NULL,
  datetime TIMESTAMP NOT NULL,
  valor FLOAT,
  UNIQUE KEY idx_sensor_variable_ubicacion (id_sensor, id_variable, id_ubicacion),
  FOREIGN KEY (id_ubicacion) REFERENCES UBICACION(id_ubicacion),
  FOREIGN KEY (id_baliza) REFERENCES BALIZA(id_baliza),
  FOREIGN KEY (id_sensor) REFERENCES SENSOR(id_sensor),
  FOREIGN KEY (id_variable) REFERENCES VARIABLE(id_variable)
);
```

LECTURA_HISTORICO
```sql
CREATE TABLE LECTURA_HISTORICO (
  id INT PRIMARY KEY AUTO_INCREMENT,
  id_ubicacion INT NOT NULL,
  id_baliza INT NOT NULL,
  id_sensor INT NOT NULL,
  id_variable INT NOT NULL,
  datetime TIMESTAMP NOT NULL,
  valor FLOAT,
  INDEX idx_ubicacion_datetime (id_ubicacion, datetime),
  FOREIGN KEY (id_ubicacion) REFERENCES UBICACION(id_ubicacion),
  FOREIGN KEY (id_baliza) REFERENCES BALIZA(id_baliza),
  FOREIGN KEY (id_sensor) REFERENCES SENSOR(id_sensor),
  FOREIGN KEY (id_variable) REFERENCES VARIABLE(id_variable)
);
```
### insercion de datos
Inserci√≥n de datos (batch o trigger)
Cuando llegan nuevas lecturas:

Se insertan en LECTURA_HISTORICO.

Se actualiza o reemplaza el valor de LECTURA_ACTUAL.

Esto puedes hacerlo desde tu backend o mediante un TRIGGER (menos flexible para datos masivos, pero funcional).

Ejemplo en backend (pseudoc√≥digo SQL)
```sql
-- 1. Insertar en hist√≥rico
INSERT INTO LECTURA_HISTORICO (id_ubicacion, id_baliza, id_sensor, id_variable, datetime, valor)
VALUES (..., ..., ..., ..., ..., ...);

-- 2. Reemplazar la lectura actual
INSERT INTO LECTURA_ACTUAL (id_ubicacion, id_baliza, id_sensor, id_variable, datetime, valor)
VALUES (..., ..., ..., ..., ..., ...)
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime);
El ON DUPLICATE KEY UPDATE funciona porque tienes la restricci√≥n UNIQUE (id_sensor, id_variable, id_ubicacion).
```
---

### consultas

Agrupamiento por ubicaci√≥n
Ahora puedes hacer consultas como:

```sql
-- √öltimas lecturas agrupadas por ubicaci√≥n
SELECT 
  id_ubicacion, 
  COUNT(*) as total_lecturas, 
  AVG(valor) as promedio_valor
FROM LECTURA_ACTUAL
GROUP BY id_ubicacion;
```
O si necesitas todo el detalle:

```sql
SELECT * 
FROM LECTURA_ACTUAL 
WHERE id_ubicacion = 3;
```
Y si necesitas comparar contra el hist√≥rico:

```sql
SELECT * 
FROM LECTURA_HISTORICO 
WHERE id_ubicacion = 3 
  AND datetime BETWEEN NOW() - INTERVAL 10 MINUTE AND NOW();
```

### particiones

üöÄ Optimizaci√≥n adicional
√çndices en `LECTURA_HISTORICO` por `(id_ubicacion, datetime)` ‚Üí muy √∫til para filtros por ubicaci√≥n y tiempo.

Particionado en LECTURA_HISTORICO:

Por fecha `(RANGE(datetime))`: √∫til si vas a consultar por per√≠odos.

Por ubicaci√≥n `(LIST(id_ubicacion))`: √∫til si consultas ubicaci√≥n por ubicaci√≥n.

Ejemplo:

```sql
CREATE TABLE LECTURA_HISTORICO_PART (
  -- columnas
)
PARTITION BY RANGE (YEAR(datetime)) (
  PARTITION p2024 VALUES LESS THAN (2025),
  PARTITION p2025 VALUES LESS THAN (2026)
);
```
### vistas?
Vistas si quieres un resumen por ubicaci√≥n para evaluaciones de seguridad:

```sql
CREATE VIEW vista_estado_actual_por_ubicacion AS
SELECT id_ubicacion, AVG(valor) as media, MAX(datetime) as ultima
FROM LECTURA_ACTUAL
GROUP BY id_ubicacion;
```

### ‚úÖConclusi√≥n
Tu idea es s√≥lida y la estrategia sugerida es:

LECTURA_ACTUAL: estado actual, consultas r√°pidas.

LECTURA_HISTORICO: almac√©n completo, an√°lisis.

Operaciones r√°pidas con ON DUPLICATE KEY UPDATE.

Agrupamientos por ubicaci√≥n son simples con GROUP BY id_ubicacion.

---
### UNIQUE KEY (id_sensor, id_varaible, id_ubicaci√≥n)
¬øQu√© es UNIQUE KEY (id_sensor, id_variable, id_ubicacion)?
Esta clave √∫nica:

Impide que haya m√°s de un registro con la misma combinaci√≥n de:

Sensor

Variable

Ubicaci√≥n

Se utiliza para que al insertar una lectura nueva del mismo sensor + variable + ubicaci√≥n, se reemplace la anterior sin duplicar.

Adem√°s, crea autom√°ticamente un √≠ndice compuesto, lo que hace que:

Buscar por id_sensor, id_variable y/o id_ubicacion sea mucho m√°s r√°pido.

Las b√∫squedas y actualizaciones sean eficientes (optimizaci√≥n de consultas).


Este √≠ndice:

sql
Copiar c√≥digo
UNIQUE KEY idx_sensor_variable_ubicacion (id_sensor, id_variable, id_ubicacion)
dice: ‚ùó"No puedes tener m√°s de una fila con la combinaci√≥n (1, 1, 10) o (2, 1, 10)."

---

### ejemplo?

Ejemplo completo con dos ubicaciones y varios sensores/variables
Supongamos que tienes:

2 ubicaciones: 10 y 20

2 sensores: 1 y 2

2 variables: 1 (Temperatura) y 2 (Humedad)

üü¢ Tabla LECTURA_ACTUAL con UNIQUE (id_sensor, id_variable, id_ubicacion)
```sql
CREATE TABLE LECTURA_ACTUAL (
    id_lectura INT PRIMARY KEY,
    id_sensor INT,
    id_variable INT,
    id_ubicacion INT,
    valor FLOAT,
    datetime TIMESTAMP,
    UNIQUE (id_sensor, id_variable, id_ubicacion)
);
```
üîÑ Simulaci√≥n de inserciones (usando ON DUPLICATE para mantener actualizadas)
```sql
-- Lectura inicial para sensor 1, variable 1 en ubicaci√≥n 10
INSERT INTO LECTURA_ACTUAL VALUES (1, 1, 1, 10, 22.0, '2025-05-19 10:00:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Llega una nueva lectura m√°s reciente para esa combinaci√≥n
INSERT INTO LECTURA_ACTUAL VALUES (2, 1, 1, 10, 23.5, '2025-05-19 10:05:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Humedad en ubicaci√≥n 10
INSERT INTO LECTURA_ACTUAL VALUES (3, 1, 2, 10, 65.0, '2025-05-19 10:06:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Temperatura en ubicaci√≥n 20
INSERT INTO LECTURA_ACTUAL VALUES (4, 2, 1, 20, 24.8, '2025-05-19 10:07:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);

-- Humedad en ubicaci√≥n 20
INSERT INTO LECTURA_ACTUAL VALUES (5, 2, 2, 20, 60.0, '2025-05-19 10:08:00')
ON DUPLICATE KEY UPDATE valor = VALUES(valor), datetime = VALUES(datetime), id_lectura = VALUES(id_lectura);
üìä Resultado final de la tabla LECTURA_ACTUAL
id_lectura	id_sensor	id_variable	id_ubicacion	valor	datetime
2	1	1	10	23.5	2025-05-19 10:05:00
3	1	2	10	65.0	2025-05-19 10:06:00
4	2	1	20	24.8	2025-05-19 10:07:00
5	2	2	20	60.0	2025-05-19 10:08:00
```
üî∏ Como ves: solo hay una fila por cada combinaci√≥n de sensor + variable + ubicaci√≥n.

---

üéØ Objetivo
Para cada combinaci√≥n de ubicaci√≥n y variable, quieres:

Obtener las 3 √∫ltimas lecturas (ordenadas por datetime)

Calcular la media de esas 3 lecturas (valor)

Hacerlo aunque haya m√°s o menos de 3 registros

Si hay menos de 3, se usa la media de los disponibles.

---

### Evaluaciones con ventana deslizante

Ejemplo SQL para evaluar dentro de tu l√≥gica:

```sql
SELECT id_sensor, id_variable, id_ubicacion,
       AVG(valor) AS media_ultimos_3
FROM (
    SELECT id_sensor, id_variable, id_ubicacion, valor,
           ROW_NUMBER() OVER (
               PARTITION BY id_sensor, id_variable, id_ubicacion
               ORDER BY timestamp DESC
           ) as rn
    FROM HISTORICO_LECTURAS
) sub
WHERE rn <= 3
GROUP BY id_sensor, id_variable, id_ubicacion;
```
Este c√°lculo lo puedes aplicar dentro de un procedimiento o script que se ejecute cada 10 segundos. Tambi√©n puedes usar una vista materializada si usas PostgreSQL o engines que lo soporten.

_____________

### a√±adir centroide a area y conexion, origen en planta

üóÑÔ∏è Resumen de cambios en la base de datos
Para poder almacenar tambi√©n la posici√≥n de cada √°rea y puerta, basta con a√±adir unos pocos campos a tus tablas:

sql
Copiar c√≥digo
-- En AREA, guardamos el centroide y opcionalmente su caja envolvente:
ALTER TABLE AREA
  ADD COLUMN centroid_x DOUBLE PRECISION,
  ADD COLUMN centroid_y DOUBLE PRECISION,
  ADD COLUMN min_x DOUBLE PRECISION,
  ADD COLUMN min_y DOUBLE PRECISION,
  ADD COLUMN max_x DOUBLE PRECISION,
  ADD COLUMN max_y DOUBLE PRECISION;

-- En CONEXION (puertas), guardamos el punto de inserci√≥n:
ALTER TABLE CONEXION
  ADD COLUMN pos_x DOUBLE PRECISION,
  ADD COLUMN pos_y DOUBLE PRECISION;

-- En PLANTA, opcionalmente el origen de coordenadas:
ALTER TABLE PLANTA
  ADD COLUMN origin_x DOUBLE PRECISION,
  ADD COLUMN origin_y DOUBLE PRECISION;

  ___

Vale quiero ir la siguiente bloque, quiero que recuerdes todo lo que hablamos y me digas qeu es lo que recuerdas para comfinar, a√±adir o elimiinar ideas.

Te paso el codigo mermaid de referentia.

con respecto alas balizas tambien me gustaria conoces u coordenada xy y lo otro igual que hemos hecho antes asi podemos situarlas en el espacio.

 %% --- Bloque 2: Sensores y Lecturas ---
    %% Esto modela los sensores y el estado en tiempo real de las mediciones, es din√°mica.
    AREA ||--o{ LECTURA : "ocurre en"
    AREA ||--o{ HISTORICO_LECTURAS : "ocurre en"
    AREA ||--o{ BALIZA : "tiene"

    BALIZA   ||--o{ BALIZA_SENSOR    : "tiene sensores"
    SENSOR   ||--|| SENSOR_VARIABLE  : "mide variables"
    SENSOR   ||--o{ BALIZA_SENSOR    : "conectado a"
    VARIABLE ||--|| SENSOR_VARIABLE  : "definida en"

    HISTORICO_LECTURAS||--o{ VARIABLE : "mide"
    HISTORICO_LECTURAS||--o{ SENSOR   : "usa"
    HISTORICO_LECTURAS||--o{ BALIZA   : "genera"

    LECTURA ||--o{ VARIABLE  : "mide"
    LECTURA ||--o{ SENSOR    : "usa"
    LECTURA ||--o{ BALIZA    : "genera"
    
    BALIZA {
      int id_baliza PK
      varchar nombre
      text descripcion
      boolean activa
      int id_area FK
    }
    SENSOR {
      int id_sensor PK
      varchar nombre
      text descripcion
      boolean activa
    }
    VARIABLE {
      int id_variable PK
      varchar nombre
      text descripcion
      varchar unidad
    }
    BALIZA_SENSOR {
      int id PK
      int id_baliza FK
      int id_sensor FK
      text descripcion
    }
    SENSOR_VARIABLE {
      int id PK
      int id_sensor FK
      int id_variable FK
      text descripcion
    }
    %% En LECTURA se almacena los ultimos valores medidos por los sensores de las balizas en cada una de las AREAS, su tama√±o es fijo, y unicamente se actualiza el valor de la variable, se podr√≠a decir que es una especie de Snapshot del estado del sistema en t√©rminos de las variables.
    %% Mi idea con esta entidad es tener las ultimas lecturas como si fuera un Live.
    LECTURA {
      int id_lectura PK
      int id_area FK
      int id_baliza FK
      int id_sensor FK
      int id_variable FK
      float valor
      datetime timestamp
    }
    %% Como LECTURA es una tabla est√°tica, para no perd√©r los datos con cada nueva lectura, se deben ir volcando con cada nueva LECTURA en su HISTORICO, se vuelca al mismo tiempo que se obtiene la lectura de tal modo que los ultimos registros de LECTURA y su HISTORICO son los mismos
    %% Mi idea con esta entidad es poder tener un registro en el tiempo de como han evolucionado las variabes (T¬∫, CO2, Humo) y poder auditar lo que ha ocurrido, tambien de obtener una media de las ultimas lecturas, como una media de temperatura, el incremento, etc, para poder tener control sobre como evolucionan y detectar patrones.
    HISTORICO_LECTURAS {
      int id_historico_lectura PK
      int id_area FK
      int id_baliza FK
      int id_sensor FK
      int id_variable FK
      float valor
      datetime timestamp
    }

### Orden al crear la base de datos

üß™ ¬øQu√© orden seguir?
Cuando montas toda la base desde cero, este es el orden l√≥gico:

Crear la base de datos (si a√∫n no existe).

Crear las tablas base.

Crear las funciones de trigger (son funciones como set_pos_from_geom()).

Crear los triggers que usan esas funciones.

Crear las vistas.

Crear los √≠ndices.

üìç En pgAdmin
Abre tu base en pgAdmin.

Clic derecho en el esquema ‚Üí Query Tool.

Pega el bloque SQL (funci√≥n, trigger o vista).

Ejecuta (F5 o bot√≥n del rayo).

Puedes hacer uno por uno o pegar un archivo .sql completo con todo junto (si el orden es correcto).

‚úÖ Recomendaci√≥n
Si est√°s haciendo un proyecto serio (aunque sea personal):

‚ú® Crea tu estructura.sql desde ya.

A√±ade todos tus CREATE TABLE, CREATE FUNCTION, CREATE TRIGGER, CREATE VIEW, CREATE INDEX.

Usa IF NOT EXISTS y CREATE OR REPLACE para que lo puedas ejecutar varias veces sin miedo.

___

/sql/
‚îú‚îÄ‚îÄ 00_create_database.sql       ‚Üê (opcional) crea la base si no existe
‚îú‚îÄ‚îÄ 01_schemas.sql               ‚Üê schemas si usas (ej: `public`, `gis`, etc.)
‚îú‚îÄ‚îÄ 02_tables_core.sql           ‚Üê tablas principales (usuarios, √°reas, etc.)
‚îú‚îÄ‚îÄ 03_tables_relacionales.sql   ‚Üê tablas intermedias, relaciones N:M
‚îú‚îÄ‚îÄ 04_views.sql                 ‚Üê vistas
‚îú‚îÄ‚îÄ 05_functions.sql             ‚Üê funciones PL/pgSQL (ej: triggers)
‚îú‚îÄ‚îÄ 06_triggers.sql              ‚Üê triggers conectados a funciones
‚îú‚îÄ‚îÄ 07_indexes.sql               ‚Üê √≠ndices
‚îú‚îÄ‚îÄ 08_sample_data.sql           ‚Üê datos de prueba (si quieres)

__

/sql/
‚îú‚îÄ‚îÄ 02_tablas_autenticacion.sql
‚îú‚îÄ‚îÄ 03_tablas_espaciales.sql
‚îú‚îÄ‚îÄ 04_tablas_eventos.sql

/sql/
‚îú‚îÄ‚îÄ 00_create_database.sql
‚îú‚îÄ‚îÄ 01_extensions.sql                    ‚Üê PostGIS y otras
‚îú‚îÄ‚îÄ 02_schemas.sql                       ‚Üê Si usas varios esquemas
‚îú‚îÄ‚îÄ 03_tablas_autenticacion.sql
‚îú‚îÄ‚îÄ 04_tablas_espaciales.sql
‚îú‚îÄ‚îÄ 05_tablas_eventos.sql
‚îú‚îÄ‚îÄ 06_particiones_eventos.sql          ‚Üê Si particionas por a√±os
‚îú‚îÄ‚îÄ 07_functions_triggers.sql
‚îú‚îÄ‚îÄ 08_views.sql
‚îú‚îÄ‚îÄ 09_indexes.sql
‚îú‚îÄ‚îÄ 10_datos_prueba.sql

‚úÖ Recomendaci√≥n final
Necesidad	Soluci√≥n
Base con muchas tablas	‚úÖ Divide por bloques (.sql por m√≥dulo)
Evitar errores al ejecutar varias veces	‚úÖ Usa IF NOT EXISTS, OR REPLACE
Crear base completa desde cero	‚úÖ Usa script con orden y comentarios
Tener futuro control de versiones	‚úÖ Guarda en Git (estructura + cambios)
Particiones	‚úÖ Dentro del archivo de su tabla, o aparte

___

üì¶ Conclusi√≥n
Tu estructura es totalmente v√°lida y no es necesario ‚Äúrepensarla‚Äù ahora. Pero puedes:

Mejora	Acci√≥n sugerida
Mejor organizaci√≥n	‚úÖ Separar en un bloque tablas_usuarios.sql
Buen rendimiento futuro	‚úÖ A√±adir √≠ndices geom, timestamp, etc.
Mayor claridad	‚úÖ A√±adir comentarios en cada tabla
Automatizaci√≥n o l√≥gica espacial	‚úÖ Usar PostGIS tambi√©n aqu√≠, si guardas ubicaciones

---

‚úÖ Conclusi√≥n
Quieres...	Usa...
Solo usar NetworkX o exportar a Python	pos_x, pos_y
Mostrar en plano GIS / QGIS / mapas interactivos	GEOMETRY(POINT)
Hacer an√°lisis espacial (distancias, zonas)	GEOMETRY(POINT)
Tener ambos	Usa geometry(POINT) y calcula pos_x, pos_y como campos o vista

---
### entidda door-to-door para tener distancias (aristas)
ALTER TABLE door_to_door
ADD COLUMN distancia_metros DOUBLE PRECISION GENERATED ALWAYS AS (
  ST_Length(geom)
) STORED;

üëâ Esto te da una columna siempre sincronizada con la geometr√≠a. Perfecto para usar como peso en NetworkX (weight).

---
### modelar door-to-door
‚úÖ Entonces s√≠ necesitas modelar door-to-door aparte
Porque est√°s pasando de:

üî≥ √Årea-a-√°rea (conceptual, a nivel de recinto)
‚Üí conexiones (ya la tienes)

a

üö™ Puerta-a-puerta (f√≠sico, en el plano)
‚Üí door_to_door (que t√∫ defines)

### Sobre los espacios en forma de L T U

Dividirlo en rectangulos conectados.
L: dos rectangulos con una conexion |_
T: dos rectangulos con una conexion |-
U: tres rectangulos con dos conexiones |_|

---
BD: PostgreSQL + PostGIS con la z, para tenerlo ya modelado pero trabajare en 2D.

CRS: Voy a trabajar con un edificion en principio ficticio y creado por mi el SRID creo que es mejor que sea EPSG:3857 (me interesa algo compatible con web de cara al dashboard) de momento no ncesito coordenadas reales. No cambies el SRID ni uses otro CRS salvo que yo lo indique expresamente.

Namespacing: dos esquemas de BD: indoorgml_core y indoorgml_nav, de momento esos mas adelanto los aumentar√©.

IDs: No quiero uuid, lo que voy a quere hacer es usar id numerico PK junto co un code alfa-num√©rico de forma que sea auto explicativo, para mostrar esto en dashboard y no el id numerico como clave primaria.

Sem√°ntica: declaramos en ThematicLayer.semanticExtension = true para capas donde apliques Navigation. 

id
Tabla	PK interna	code ejemplo
PrimalSpaceLayer	1	PR-01
DualSpaceLayer	1	DU-01
CellSpace	1	CS-0001
CellBoundary	1	CB-0001-0002
Node	1	ND-0001
Edge	1	ED-0001-0002
NavigableSpace	1	NS-0001
NonNavigableSpace	1	NN-0001
NavigableBoundary	1	NB-0001-0002
NonNavigableBoundary	1	NNB-0001-0002
TransferSpace	1	TS-0001
ObjectSpace	1	OS-0001
Route	1	RT-2025001 (a√±o + secuencia)